{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkZNcmguluIP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import LoggingHandler, util\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIyZnA97qCI2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJnCxwLdqSdO"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ZFbZFJoHrI"
      },
      "outputs": [],
      "source": [
        "model_name = 'SALT-NLP/FLANG-Roberta'\n",
        "PATH = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-evY5zNnuwG"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel(PATH + \"CRED_with_tweets.xlsx\", dtype={'retweet_count_x_more_y': np.int32})\n",
        "\n",
        "data['idx'] = data.index\n",
        "\n",
        "dt1 = \"2022-04-30\"\n",
        "df_train = data[(data['Datetime_x']<=dt1) & (data['Datetime_y']<=dt1)]\n",
        "df_test = data[(data['Datetime_x']>dt1) & (data['Datetime_y']>dt1)]\n",
        "\n",
        "df_train = df_train.reset_index(drop=True).drop(['Datetime_x', 'Datetime_y'], axis = 1)\n",
        "df_test = df_test.reset_index(drop=True).drop(['Datetime_x', 'Datetime_y'], axis = 1)\n",
        "\n",
        "\n",
        "model_save_path = PATH + 'crossencoder_whitehouse_flangroberta_snli_sbert_with_claude'\n",
        "df_train['blank'] = ' '\n",
        "df_test['blank'] = ' '\n",
        "\n",
        "df_train['cleaned_tweet_text_x'] = df_train['cleaned_tweet_text_x'] + df_train['blank'] + df_train['claude_x_cleaned']\n",
        "df_train['cleaned_tweet_text_y'] = df_train['cleaned_tweet_text_y'] + df_train['blank'] + df_train['claude_y_cleaned'] \n",
        "df_test['cleaned_tweet_text_x'] = df_test['cleaned_tweet_text_x'] + df_test['blank'] + df_test['claude_x_cleaned']\n",
        "df_test['cleaned_tweet_text_y'] = df_test['cleaned_tweet_text_y'] + df_test['blank'] + df_test['claude_y_cleaned']\n",
        "\n",
        "\n",
        "df_train = df_train[['cleaned_tweet_text_x', 'cleaned_tweet_text_y', 'retweet_count_x_more_y', 'idx']].rename(columns = {'cleaned_tweet_text_x':'sentence1', 'cleaned_tweet_text_y':'sentence2', 'retweet_count_x_more_y':'label'}).copy()\n",
        "df_test = df_test[['cleaned_tweet_text_x', 'cleaned_tweet_text_y', 'retweet_count_x_more_y', 'idx']].rename(columns = {'cleaned_tweet_text_x':'sentence1', 'cleaned_tweet_text_y':'sentence2', 'retweet_count_x_more_y':'label'}).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY8kkYFJnUHj"
      },
      "outputs": [],
      "source": [
        "num_labels = df_train['label'].nunique()\n",
        "\n",
        "train_samples = []\n",
        "for sent1, sent2, label in zip(df_train['sentence1'], df_train['sentence2'], df_train['label']):\n",
        "  train_samples.append(InputExample(texts=[sent1, sent2], label=label))\n",
        "\n",
        "\n",
        "dev_samples = []\n",
        "for sent1, sent2, label in zip(df_test['sentence1'], df_test['sentence2'], df_test['label']):\n",
        "  dev_samples.append(InputExample(texts=[sent1, sent2], label=label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnTmqpqgn119"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 16\n",
        "num_epochs = 20\n",
        "\n",
        "\n",
        "#Define our CrossEncoder model. We use distilroberta-base as basis and setup it up to predict 3 labels\n",
        "model = CrossEncoder(model_name, num_labels=num_labels)\n",
        "\n",
        "#We wrap train_samples, which is a list ot InputExample, in a pytorch DataLoader\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
        "\n",
        "#During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
        "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(dev_samples, name='cross_encoder_flangroberta')\n",
        "\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataloader=train_dataloader,\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=10000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V4GHb7ypXrR"
      },
      "outputs": [],
      "source": [
        "scores = model.predict([(sent1, sent2) for sent1, sent2 in zip(df_test['sentence1'].tolist(), df_test['sentence2'].tolist())])\n",
        "\n",
        "#Convert scores to labels\n",
        "predicted_labels = [score_max for score_max in scores.argmax(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-doa6soqrYT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b26052-6544-4465-8f17-4890e6e4271e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7194719471947195\n",
            "0.7310126582278481\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(accuracy_score(df_test['label'].tolist(), predicted_labels))\n",
        "print(f1_score(df_test['label'].tolist(), predicted_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}